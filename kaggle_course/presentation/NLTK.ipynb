{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "<br>\n",
    "Generally, the NLP field is characterized by the following terms:\n",
    "<br><br>\n",
    "<b>Tokenization</b>\t- segmenting text into words, punctuations marks etc.\n",
    "<br>\n",
    "<b>Part-of-speech (POS) Tagging</b>\t- Assigning word types to tokens, like verb or noun.\n",
    "<br>\n",
    "<b>Dependency Parsing</b> - Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "<br>\n",
    "<b>Lemmatization</b> - Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"playing\" is \"play\".\n",
    "<br>\n",
    "<b>Sentence Boundary Detection (SBD)</b> - Finding and segmenting individual sentences.\n",
    "\n",
    "### Differences between NLTK and spaCy\n",
    "NLTK offers some of the same functionality as spaCy. Although originally developed for teaching and research, its longevity and stability has resulted in a large number of industrial users. It's the main alternative to spaCy for tokenization and sentence segmentation. In comparison to spaCy, NLTK takes a much more \"broad church\" approach â€“ so it has some functions that spaCy doesn't provide, at the expense of a bit more clutter to sift through. spaCy is also much more performance-focussed than NLTK: where the two libraries provide the same functionality, spaCy's implementation will usually be faster and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) a free and open-source leading platform for building Python programs to work with human language data.\n",
    "<br>\n",
    "It is written in Python and provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers, etc.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The First thing that we need to do is to load the Quora dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_data = train_data[:400000]\n",
    "\n",
    "train_text = train_data['question_text'].values\n",
    "train_labels = train_data['target'].values\n",
    "\n",
    "test_text = test_data['question_text'].values\n",
    "test_qid = test_data['qid'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311184</th>\n",
       "      <td>3cf3f6d126ce410d7b58</td>\n",
       "      <td>Why don't we Hindus hate ex-president Abdul Ka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226106</th>\n",
       "      <td>2c36fd62c059967a2fee</td>\n",
       "      <td>Who do also hate the new unskippable ads on Yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50491</th>\n",
       "      <td>09e5a49af0d7923ff008</td>\n",
       "      <td>Are Americans aware of their global unpopularity?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138993</th>\n",
       "      <td>1b3737b7501354682449</td>\n",
       "      <td>Which apps can be used on Android to see world...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321911</th>\n",
       "      <td>3f16506befbca637cd09</td>\n",
       "      <td>What should Caucasians do to reduce fertility ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122308</th>\n",
       "      <td>17f041a8626c1f19934b</td>\n",
       "      <td>Is there a version of Frank Castle where he fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190804</th>\n",
       "      <td>254e7e77f4efa0d081a2</td>\n",
       "      <td>Why is Malayalam new year called \"vishu\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72181</th>\n",
       "      <td>0e2688cb6136e20d9e94</td>\n",
       "      <td>Which instruction is used to selectively mask ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175406</th>\n",
       "      <td>224bdb1a11a98141a4c3</td>\n",
       "      <td>Are construction delays common with Emaar off ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181432</th>\n",
       "      <td>2374f1641661814436c2</td>\n",
       "      <td>Which will be the best course online for AWS D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "311184  3cf3f6d126ce410d7b58   \n",
       "226106  2c36fd62c059967a2fee   \n",
       "50491   09e5a49af0d7923ff008   \n",
       "138993  1b3737b7501354682449   \n",
       "321911  3f16506befbca637cd09   \n",
       "122308  17f041a8626c1f19934b   \n",
       "190804  254e7e77f4efa0d081a2   \n",
       "72181   0e2688cb6136e20d9e94   \n",
       "175406  224bdb1a11a98141a4c3   \n",
       "181432  2374f1641661814436c2   \n",
       "\n",
       "                                            question_text  target  \n",
       "311184  Why don't we Hindus hate ex-president Abdul Ka...       1  \n",
       "226106  Who do also hate the new unskippable ads on Yo...       0  \n",
       "50491   Are Americans aware of their global unpopularity?       1  \n",
       "138993  Which apps can be used on Android to see world...       0  \n",
       "321911  What should Caucasians do to reduce fertility ...       1  \n",
       "122308  Is there a version of Frank Castle where he fi...       0  \n",
       "190804          Why is Malayalam new year called \"vishu\"?       0  \n",
       "72181   Which instruction is used to selectively mask ...       0  \n",
       "175406  Are construction delays common with Emaar off ...       0  \n",
       "181432  Which will be the best course online for AWS D...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we can do with the data is to convert all the letters to lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train_text = [token.lower() for token in train_text]\n",
    "test_text = [token.lower() for token in test_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a way to split text into tokens. These tokens could be paragraphs, sentences, or individual words. NLTK provides a number of tokenizers in the tokenize module.\n",
    "\n",
    "The text is first tokenized into sentences using the <b>PunktSentenceTokenizer</b>, hen each sentence is tokenized into words using 4 different word tokenizers:\n",
    "\n",
    "<b>TreebankWordTokenizer</b>\n",
    "\n",
    "<b>WordPunctTokenizer</b>\n",
    "\n",
    "<b>PunctWordTokenizer</b>\n",
    "\n",
    "<b>WhitespaceTokenizer</b>\n",
    "\n",
    "By default, NLTK uses the TreebankWordTokenizer, which uses regular expressions to tokenize text and it asumes that the text has already been splitted into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = [word_tokenize(i) for i in train_text]\n",
    "tokenized_words_test = [word_tokenize(i) for i in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_words_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-25afae01eb38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenized_words_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_words_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenized_words_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_words_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_words_train' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('tokenized_words_train', tokenized_words_train)\n",
    "np.save('tokenized_words_test', tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = np.load('tokenized_words_train.npy')\n",
    "tokenized_words_test = np.load('tokenized_words_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>isalpha()</b> is a built-in Python method which checks if a string contains only alphabethical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "tokenized_words_train = [[word for word in sent if word.isalpha()] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ASCII characters\n",
    "tokenized_words_train_flat = [item for sublist in tokenized_words_train for item in sublist]\n",
    "\n",
    "cleaned_data = [re.sub(r'[^\\x00-\\x7f]', r'', word) for word in tokenized_words_train_flat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>FreqDist</b> function returns the frequency distribution for the outcomes of an experiment.\n",
    "\n",
    "A frequency distribution can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low frequency words\n",
    "freq_words = FreqDist(cleaned_data)\n",
    "\n",
    "cleaned_data = { key : value for key, value in freq_words.items() if value > 10 }\n",
    "\n",
    "filtered_data = []\n",
    "temp_array = []\n",
    "\n",
    "for sent in tokenized_words_train:\n",
    "    for word in sent:\n",
    "        if word in cleaned_data.keys():\n",
    "            temp_array.append(word)\n",
    "    filtered_data.append(temp_array)\n",
    "    temp_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also provides a list of stop-words, which are the most frequent words in a language.\n",
    "\n",
    "For example the most frequent English words are the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove those words because they appear in almost every sentence, thus won't have much impact on the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "filtered_data_no_stopwords = []\n",
    "temp_array = []\n",
    "\n",
    "for sent in filtered_data:\n",
    "    for word in sent:\n",
    "        if word not in stop_words:\n",
    "            temp_array.append(word)\n",
    "    filtered_data_no_stopwords.append(temp_array)\n",
    "    temp_array = []\n",
    "\n",
    "filtered_data = filtered_data_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bag-of-Words</b> is a very intuitive approach for converting the words into numerical values.\n",
    "\n",
    "The approach follows 3 steps:\n",
    "\n",
    "<ol>\n",
    "<li>Splitting the documents into tokens</li>\n",
    "<li>Assigning a weight to each token proportional to the frequency with which it shows up in the document and/or corpora.</li>\n",
    "<li>Creating a document-term matrix with each row representing a document and each column addressing a token.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "The <b>Count Vectorizer</b> counts the number of times a token shows up in the document and uses this value as its weight.\n",
    "\n",
    "The <b>tokenizer</b> argument overrides the string tokenization step while preserving the preprocessing and n-grams generation steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer = lambda sent: sent, \n",
    "    analyzer = 'word',\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(filtered_data)\n",
    "X_test = vectorizer.transform(tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross valudation\n",
    "LR = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    LR, \n",
    "    X_train, \n",
    "    train_labels, \n",
    "    cv = 5, \n",
    "    scoring = 'f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49030520084606233"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = np.sum(scores) / len(scores)\n",
    "avg_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
