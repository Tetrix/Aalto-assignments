{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is an open-source software library for advanced natural language processing, written in Cython.\n",
    "<br>\n",
    "It's focus is on providing software for production usage and excels at large-scale information extraction tasks.\n",
    "<br>\n",
    "\n",
    "spaCy provides the following key features:\n",
    "<ol>\n",
    "    <li>Non-destructive tokenization</li>\n",
    "    <li>Named entity recognition</li>\n",
    "    <li>\"Alpha tokenization\" support for over 25 languages</li>\n",
    "    <li>Statistical models models for 8 languages</li>\n",
    "    <li>Pre-trained word vectors</li>\n",
    "    <li>Part-of-speech tagging</li>\n",
    "    <li>Labelled dependency parsing</li>\n",
    "    <li>Syntax-driven sentence segmentation</li>\n",
    "    <li>Text classification</li>\n",
    "    <li>Built-in visualizers for syntax and named entities</li>\n",
    "    <li>Deep learning integration</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_data = train_data[:400000]\n",
    "\n",
    "train_text = train_data['question_text'].values\n",
    "train_labels = train_data['target'].values\n",
    "\n",
    "test_text = test_data['question_text'].values\n",
    "test_qid = test_data['qid'].values\n",
    "\n",
    "# load the Spacy model\n",
    "spacy_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we can do with the data is to convert all the letters to lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train_text = [token.lower() for token in train_text]\n",
    "test_text = [token.lower() for token in test_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call <b>spacy_model</b> on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
    "\n",
    "<img src=\"processing.png\" alt=\"processing\">\n",
    "\n",
    "<br>\n",
    "where <b>tagger</b> assigns pat-of-speech tags, <b>parser</b> assigns dependency labels and <b>ner</b> detects and labels named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the task of splitting a text into meaningful segments, called tokens.\n",
    "<br>\n",
    "The input to the tokenizer is a unicode text, and the output is a Doc object, which is a sequence of tokens.\n",
    "\n",
    "SpaCy introduces a novel tokenization algorithm, that gives a better balance between performance, ease of definition, and ease of alignment into the original string.\n",
    "<br><br>\n",
    "The tokenization algorithm is done in the following steps:\n",
    "<ol>\n",
    "    <li>Iterate over space-separated substrings.</li>\n",
    "    <li>Check whether we have an explicitly defined rule for this substring. If we do, use it.</li>\n",
    "    <li>Otherwise, try to consume a prefix.</li>\n",
    "    <li>If we consumed a prefix, go back to the beginning of the loop, so that special-cases always get priority.</li>\n",
    "    <li>If we didn't consume a prefix, try to consume a suffix.</li>\n",
    "    <li>If we can't consume a prefix or suffix, look for \"infixes\" — stuff like hyphens etc.</li>\n",
    "    <li>Once we can't consume any more of the string, handle it as a single token.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(spacy_model.vocab)\n",
    "\n",
    "tokenized_words_train = [tokenizer(sent) for sent in train_text]\n",
    "tokenized_words_test = [tokenizer(sent) for sent in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tokenized_words_spacy_train', tokenized_words_train)\n",
    "np.save('tokenized_words_spacy_test', tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = np.load('tokenized_words_spacy_train.npy')\n",
    "tokenized_words_test = np.load('tokenized_words_spacy_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[how did quebec nationalists see their province as a nation in the 1960s?,\n",
       " do you have an adopted dog, how would you encourage people to adopt and not shop?,\n",
       " why does velocity affect time? does velocity affect space geometry?,\n",
       " how did otto von guericke used the magdeburg hemispheres?,\n",
       " can i convert montra helicon d to a mountain bike by just changing the tyres?]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "tokenized_words_train = [[word for word in sent if word.is_alpha] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[how, did, quebec, nationalists, see, their, province, as, a, nation, in, the]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ASCII characters\n",
    "tokenized_words_train_flat = [item for sublist in tokenized_words_train for item in sublist]\n",
    "\n",
    "cleaned_data = [re.sub(r'[^\\x00-\\x7f]', r'', word.text) for word in tokenized_words_train_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low-frequency words\n",
    "freq_words = Counter(cleaned_data)\n",
    "\n",
    "cleaned_data = { key : value for key, value in freq_words.items() if value > 10 }\n",
    "\n",
    "filtered_data = []\n",
    "temp_array = []\n",
    "\n",
    "for sent in tokenized_words_train:\n",
    "    for word in sent:\n",
    "        if word.text in cleaned_data.keys():\n",
    "            temp_array.append(word)\n",
    "    filtered_data.append(temp_array)\n",
    "    temp_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-52c046d9462e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "# filtered_data = [[word for word in sent if word.is_stop == False] for sent in tokenized_words_train]\n",
    "\n",
    "filtered_data_no_stopwords = [[word for word in sent if word.is_stop == False] for sent in filtered_data]\n",
    "\n",
    "filtered_data = filtered_data_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[quebec, nationalists, province, nation],\n",
       " [adopted, encourage, people, adopt],\n",
       " [velocity, affect, velocity, affect, space],\n",
       " [otto, von],\n",
       " [convert, d, mountain, bike, changing]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "filtered_data_no_lemma = [[word.lemma_ for word in sent] for sent in filtered_data]\n",
    "\n",
    "filtered_data = filtered_data_no_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quebec', 'nationalist', 'province', 'nation'],\n",
       " ['adopt', 'encourage', 'people', 'adopt'],\n",
       " ['velocity', 'affect', 'velocity', 'affect', 'space'],\n",
       " ['otto', 'von'],\n",
       " ['convert', 'have', 'mountain', 'bike', 'change']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "<br>\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "<br>\n",
    "It tries to make words with similar context occupy close spatial positions.\n",
    "<br><br>\n",
    "The Word2Vec model can be obtained using 2 techniques: \n",
    "<ol>\n",
    "    <li>Skip Gram</li>\n",
    "    <li>Common Bag Of Words (CBOW)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_wiki = KeyedVectors.load_word2vec_format('../data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[embed_wiki[word] for word in sent if word in embed_wiki.vocab] for sent in filtered_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of the vectors\n",
    "X_avg = []\n",
    "\n",
    "for vector in X:\n",
    "    if len(vector) >= 1:\n",
    "        X_avg.append(np.mean(vector))\n",
    "    else:\n",
    "        X_avg.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_avg = np.array(X_avg)\n",
    "X_avg = X_avg.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00032158],\n",
       "       [-0.00506292],\n",
       "       [-0.004897  ],\n",
       "       [ 0.001083  ],\n",
       "       [-0.00152267]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross valudation\n",
    "LR = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    LR, \n",
    "    X_avg, \n",
    "    train_labels, \n",
    "    cv = 5, \n",
    "    scoring = 'f1_macro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4839921463708913"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = np.sum(scores) / len(scores)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_text[:100]\n",
    "\n",
    "tokenized_text = [spacy_model(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tagging.png\" alt=\"tagging\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech tagging\n",
    "tagged_text = [{word : word.tag_ for word in sent} for sent in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads.\n",
    "<br>\n",
    "<img src=\"parsing.png\" alt=\"parsing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get noun chunks\n",
    "\n",
    "sample_sent = tokenized_text[1]\n",
    "\n",
    "text = []\n",
    "root = []\n",
    "root_dep = []\n",
    "root_head = []\n",
    "\n",
    "for chunk in sample_sent.noun_chunks:\n",
    "    text.append(chunk.text)\n",
    "    root.append(chunk.root.text)\n",
    "    root_dep.append(chunk.root.dep_)\n",
    "    root_head.append(chunk.root.head.text)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "        'TEXT': text, \n",
    "        'ROOT.TEXT': root, \n",
    "        'ROOT.DEP': root_dep, \n",
    "        'ROOT.HEAD.TEXT': root_head\n",
    "    })\n",
    "\n",
    "df = df[['TEXT', 'ROOT.TEXT', 'ROOT.DEP', 'ROOT.HEAD.TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you have an adopted dog, how would you encourage people to adopt and not shop?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>ROOT.TEXT</th>\n",
       "      <th>ROOT.DEP</th>\n",
       "      <th>ROOT.HEAD.TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an adopted dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dobj</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>encourage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>dobj</td>\n",
       "      <td>encourage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TEXT ROOT.TEXT ROOT.DEP ROOT.HEAD.TEXT\n",
       "0             you       you    nsubj           have\n",
       "1  an adopted dog       dog     dobj           have\n",
       "2             you       you    nsubj      encourage\n",
       "3          people    people     dobj      encourage"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sample_sent)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Text</b>: The original noun chunk text.\n",
    "<br>\n",
    "<b>Root text</b>: The original text of the word connecting the noun chunk to the rest of the parse.\n",
    "<br>\n",
    "<b>Root dep</b>: Dependency relation connecting the root to its head.\n",
    "<br>\n",
    "<b>Root head text</b>: The text of the root token's head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "sample_sent = 'Google was founded in 1998 in California'\n",
    "doc = spacy_model(sample_sent)\n",
    "\n",
    "text = []\n",
    "start = []\n",
    "end = []\n",
    "label = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    text.append(ent.text)\n",
    "    start.append(ent.start_char)\n",
    "    end.append(ent.end_char)\n",
    "    label.append(ent.label_)\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    'TEXT': text, \n",
    "    'START': start, \n",
    "    'END': end, \n",
    "    'LABEL': label\n",
    "})\n",
    "\n",
    "df = df[['TEXT', 'START', 'END', 'LABEL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEXT  START  END LABEL\n",
       "0      Google      0    6   ORG\n",
       "1        1998     22   26  DATE\n",
       "2  California     30   40   GPE"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between dog and cat is: 0.509429395198822\n",
      "Similarity between dog and chair is: 0.35649821162223816\n",
      "Similarity between dog and dog is: 1.0\n"
     ]
    }
   ],
   "source": [
    "text = 'dog cat chair'\n",
    "doc = spacy_model(text)\n",
    "\n",
    "token1 = doc[0]\n",
    "token2 = doc[1]\n",
    "token3 = doc[2]\n",
    "\n",
    "\n",
    "print('Similarity between {0} and {1} is: {2}'.format(token1, token2, token1.similarity(token2)))\n",
    "print('Similarity between {0} and {1} is: {2}'.format(token1, token3, token1.similarity(token3)))\n",
    "print('Similarity between {0} and {1} is: {2}'.format(token1, token1, token1.similarity(token1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
