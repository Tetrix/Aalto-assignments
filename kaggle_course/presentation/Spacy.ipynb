{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is an open-source software library for advanced natural language processing, written in Cython.\n",
    "<br>\n",
    "It's focus is on providing software for production usage and excels at large-scale information extraction tasks.\n",
    "<br>\n",
    "\n",
    "spaCy provides the following key features:\n",
    "<ol>\n",
    "    <li>Non-destructive tokenization</li>\n",
    "    <li>Named entity recognition</li>\n",
    "    <li>\"Alpha tokenization\" support for over 25 languages</li>\n",
    "    <li>Statistical models models for 8 languages</li>\n",
    "    <li>Pre-trained word vectors</li>\n",
    "    <li>Part-of-speech tagging</li>\n",
    "    <li>Labelled dependency parsing</li>\n",
    "    <li>Syntax-driven sentence segmentation</li>\n",
    "    <li>Text classification</li>\n",
    "    <li>Built-in visualizers for syntax and named entities</li>\n",
    "    <li>Deep learning integration</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_data = train_data[:400000]\n",
    "\n",
    "train_text = train_data['question_text'].values\n",
    "train_labels = train_data['target'].values\n",
    "\n",
    "test_text = test_data['question_text'].values\n",
    "test_qid = test_data['qid'].values\n",
    "\n",
    "# load the Spacy model\n",
    "spacy_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we can do with the data is to convert all the letters to lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train_text = [token.lower() for token in train_text]\n",
    "test_text = [token.lower() for token in test_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call <b>spacy_model</b> on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
    "\n",
    "<img src=\"processing.png\" alt=\"processing\">\n",
    "\n",
    "<br>\n",
    "where <b>tagger</b> assigns pat-of-speech tags, <b>parser</b> assigns dependency labels and <b>ner</b> detects and labels named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the task of splitting a text into meaningful segments, called tokens.\n",
    "<br>\n",
    "The input to the tokenizer is a unicode text, and the output is a Doc object, which is a sequence of tokens.\n",
    "\n",
    "SpaCy introduces a novel tokenization algorithm, that gives a better balance between performance, ease of definition, and ease of alignment into the original string.\n",
    "<br><br>\n",
    "The tokenization algorithm is done in the following steps:\n",
    "<ol>\n",
    "    <li>Iterate over space-separated substrings.</li>\n",
    "    <li>Check whether we have an explicitly defined rule for this substring. If we do, use it.</li>\n",
    "    <li>Otherwise, try to consume a prefix.</li>\n",
    "    <li>If we consumed a prefix, go back to the beginning of the loop, so that special-cases always get priority.</li>\n",
    "    <li>If we didn't consume a prefix, try to consume a suffix.</li>\n",
    "    <li>If we can't consume a prefix or suffix, look for \"infixes\" — stuff like hyphens etc.</li>\n",
    "    <li>Once we can't consume any more of the string, handle it as a single token.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(spacy_model.vocab)\n",
    "\n",
    "tokenized_words_train = [tokenizer(sent) for sent in train_text]\n",
    "tokenized_words_test = [tokenizer(sent) for sent in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tokenized_words_spacy_train', tokenized_words_train)\n",
    "np.save('tokenized_words_spacy_test', tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = np.load('tokenized_words_spacy_train.npy')\n",
    "tokenized_words_test = np.load('tokenized_words_spacy_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "tokenized_words_train = [[word for word in sent if word.is_alpha] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ASCII characters\n",
    "tokenized_words_train_flat = [item for sublist in tokenized_words_train for item in sublist]\n",
    "\n",
    "cleaned_data = [re.sub(r'[^\\x00-\\x7f]', r'', word.text) for word in tokenized_words_train_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low-frequency words\n",
    "freq_words = Counter(cleaned_data)\n",
    "\n",
    "cleaned_data = { key : value for key, value in freq_words.items() if value > 10 }\n",
    "\n",
    "filtered_data = []\n",
    "temp_array = []\n",
    "\n",
    "for sent in tokenized_words_train:\n",
    "    for word in sent:\n",
    "        if word.text in cleaned_data.keys():\n",
    "            temp_array.append(word)\n",
    "    filtered_data.append(temp_array)\n",
    "    temp_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "filtered_data = [[word for word in sent if word.is_stop == False] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "filtered_data = [[word.lemma_ for word in sent] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "<br>\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "<br>\n",
    "It tries to make words with similar context occupy close spatial positions.\n",
    "<br><br>\n",
    "The Word2Vec model can be obtained using 2 techniques: \n",
    "<ol>\n",
    "    <li>Skip Gram</li>\n",
    "    <li>Common Bag Of Words (CBOW)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_wiki = KeyedVectors.load_word2vec_format('../data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[embed_wiki[word] for word in sent if word in embed_wiki.vocab] for sent in filtered_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of the vectors\n",
    "X_avg = []\n",
    "\n",
    "for vector in X:\n",
    "    if len(vector) >= 1:\n",
    "        X_avg.append(np.mean(vector))\n",
    "    else:\n",
    "        X_avg.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_avg = np.array(X_avg)\n",
    "X_avg = X_avg.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross valudation\n",
    "LR = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    LR, \n",
    "    X_avg, \n",
    "    train_labels, \n",
    "    cv = 5, \n",
    "    scoring = 'f1_macro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4839921463708913"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = np.sum(scores) / len(scores)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech tagging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
