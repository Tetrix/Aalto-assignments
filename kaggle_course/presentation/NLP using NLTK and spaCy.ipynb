{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is a field that studies automatic computational processing of human languages.\n",
    "<br>\n",
    "Generally, NLP addresses the following tasks:\n",
    "<br><br>\n",
    "<b>Tokenization</b>\t- segmenting text.\n",
    "<br>\n",
    "<b>Part-of-speech (POS) Tagging</b>\t- Assigning word types to tokens, like verb or noun.\n",
    "<br>\n",
    "<b>Dependency Parsing</b> - Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "<br>\n",
    "<b>Lemmatization</b> - Assigning the base forms of words.\n",
    "<br>\n",
    "<b>Sentence Boundary Detection (SBD)</b> - Finding and segmenting individual sentences.\n",
    "\n",
    "### Differences between NLTK and spaCy\n",
    "NLTK and spaCy have many overlapping functionalities. In comparison to spaCy, NLTK takes much broader approach. NLTK suggests a variety of approaches to solve one task (person needs to know what to choose), while spaCy provides 1 approach for 1 task and this approach can use recently proposed method. SpaCy is also much more performance-focussed than NLTK.\n",
    "<br>\n",
    "Although the two libraries provide the same functionality, spaCy's implementation will usually be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) a free and open-source leading platform for building Python programs to work with human language data.\n",
    "<br>\n",
    "It is written in Python and provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers, etc.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The First thing that we need to do is to load the Quora dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_data = train_data[:400000]\n",
    "\n",
    "train_text = train_data['question_text'].values\n",
    "train_labels = train_data['target'].values\n",
    "\n",
    "test_text = test_data['question_text'].values\n",
    "test_qid = test_data['qid'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['How did Quebec nationalists see their province as a nation in the 1960s?',\n",
       "       'Do you have an adopted dog, how would you encourage people to adopt and not shop?',\n",
       "       'Why does velocity affect time? Does velocity affect space geometry?',\n",
       "       'How did Otto von Guericke used the Magdeburg hemispheres?',\n",
       "       'Can I convert montra helicon D to a mountain bike by just changing the tyres?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we can do with the data is to convert all the letters to lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train_text = [token.lower() for token in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how did quebec nationalists see their province as a nation in the 1960s?',\n",
       " 'do you have an adopted dog, how would you encourage people to adopt and not shop?',\n",
       " 'why does velocity affect time? does velocity affect space geometry?',\n",
       " 'how did otto von guericke used the magdeburg hemispheres?',\n",
       " 'can i convert montra helicon d to a mountain bike by just changing the tyres?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a way to split text into tokens. These tokens could be paragraphs, sentences, or individual words. NLTK provides a number of tokenizers in the tokenize module.\n",
    "\n",
    "The text is first tokenized into sentences using the <b>PunktSentenceTokenizer</b>, then each sentence is tokenized into words using 4 different word tokenizers:\n",
    "\n",
    "<b>TreebankWordTokenizer</b>\n",
    "\n",
    "<b>WordPunctTokenizer</b>\n",
    "\n",
    "<b>PunctWordTokenizer</b>\n",
    "\n",
    "<b>WhitespaceTokenizer</b>\n",
    "\n",
    "By default, NLTK uses the TreebankWordTokenizer, which uses regular expressions to tokenize text and it asumes that the text has already been splitted into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = [word_tokenize(i) for i in train_text]\n",
    "tokenized_words_test = [word_tokenize(i) for i in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_words_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-25afae01eb38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenized_words_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_words_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenized_words_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_words_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_words_train' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('tokenized_words_train', tokenized_words_train)\n",
    "np.save('tokenized_words_test', tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = np.load('tokenized_words_train.npy')\n",
    "tokenized_words_test = np.load('tokenized_words_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>isalpha()</b> is a built-in Python method which checks if a string contains only alphabethical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "tokenized_words_train = [[word for word in sent if word.isalpha()] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ASCII characters\n",
    "tokenized_words_train_flat = [item for sublist in tokenized_words_train for item in sublist]\n",
    "\n",
    "cleaned_data = [re.sub(r'[^\\x00-\\x7f]', r'', word) for word in tokenized_words_train_flat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>FreqDist</b> function returns the frequency distribution for the outcomes of an experiment.\n",
    "\n",
    "A frequency distribution can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low frequency words\n",
    "freq_words = FreqDist(cleaned_data)\n",
    "\n",
    "cleaned_data = { key : value for key, value in freq_words.items() if value > 10 }\n",
    "\n",
    "filtered_data = []\n",
    "temp_array = []\n",
    "\n",
    "for sent in tokenized_words_train:\n",
    "    for word in sent:\n",
    "        if word in cleaned_data.keys():\n",
    "            temp_array.append(word)\n",
    "    filtered_data.append(temp_array)\n",
    "    temp_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also provides a list of stop-words, which are the most frequent words in a language.\n",
    "\n",
    "For example the most frequent English words are the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove those words because they appear in almost every sentence, thus won't have much impact on the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Caution!</b> Removing the stop words might not be a good approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "# filtered_data_no_stopwords = []\n",
    "# temp_array = []\n",
    "\n",
    "# for sent in filtered_data:\n",
    "#     for word in sent:\n",
    "#         if word not in stop_words:\n",
    "#             temp_array.append(word)\n",
    "#     filtered_data_no_stopwords.append(temp_array)\n",
    "#     temp_array = []\n",
    "\n",
    "# filtered_data = filtered_data_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bag-of-Words</b> is a very intuitive approach for converting the words into numerical values.\n",
    "\n",
    "The approach follows 3 steps:\n",
    "\n",
    "<ol>\n",
    "<li>Splitting the documents into tokens</li>\n",
    "<li>Assigning a weight to each token proportional to the frequency with which it shows up in the document and/or corpora.</li>\n",
    "<li>Creating a document-term matrix with each row representing a document and each column addressing a token.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "The <b>Count Vectorizer</b> counts the number of times a token shows up in the document and uses this value as its weight.\n",
    "\n",
    "The <b>tokenizer</b> argument overrides the string tokenization step while preserving the preprocessing and n-grams generation steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer = lambda sent: sent, \n",
    "    analyzer = 'word',\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(filtered_data)\n",
    "X_test = vectorizer.transform(tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross valudation\n",
    "LR = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    LR, \n",
    "    X_train, \n",
    "    train_labels, \n",
    "    cv = 5, \n",
    "    scoring = 'f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = np.sum(scores) / len(scores)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is an open-source software library for advanced natural language processing, written in Cython.\n",
    "<br>\n",
    "It's focus is on providing software for production usage and excels at large-scale information extraction tasks.\n",
    "<br>\n",
    "\n",
    "spaCy provides the following key features:\n",
    "<ol>\n",
    "    <li>Non-destructive tokenization</li>\n",
    "    <li>Named entity recognition</li>\n",
    "    <li>\"Alpha tokenization\" support for over 25 languages</li>\n",
    "    <li>Pre-trained word vectors</li>\n",
    "    <li>Part-of-speech tagging</li>\n",
    "    <li>Labelled dependency parsing</li>\n",
    "    <li>Syntax-driven sentence segmentation</li>\n",
    "    <li>Text classification</li>\n",
    "    <li>Built-in visualizers for syntax and named entities</li>\n",
    "    <li>Deep learning integration</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_data = train_data[:400000]\n",
    "\n",
    "train_text = train_data['question_text'].values\n",
    "train_labels = train_data['target'].values\n",
    "\n",
    "test_text = test_data['question_text'].values\n",
    "test_qid = test_data['qid'].values\n",
    "\n",
    "# load the Spacy model\n",
    "spacy_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm ==> command to install a spaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we can do with the data is to convert all the letters to lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train_text = [token.lower() for token in train_text]\n",
    "test_text = [token.lower() for token in test_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call <b>spacy_model</b> on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
    "\n",
    "<img src=\"images/processing.png\" alt=\"processing\">\n",
    "\n",
    "<br>\n",
    "where <b>tagger</b> assigns pat-of-speech tags, <b>parser</b> assigns dependency labels and <b>ner</b> detects and labels named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the task of splitting a text into meaningful segments, called tokens.\n",
    "<br>\n",
    "The input to the tokenizer is a unicode text, and the output is a Doc object, which is a sequence of tokens.\n",
    "\n",
    "SpaCy introduces a novel tokenization algorithm, that gives a better balance between performance, ease of definition, and ease of alignment into the original string.\n",
    "<br><br>\n",
    "The tokenization algorithm is done in the following steps:\n",
    "<ol>\n",
    "    <li>Iterate over space-separated substrings.</li>\n",
    "    <li>Check whether we have an explicitly defined rule for this substring. If we do, use it.</li>\n",
    "    <li>Otherwise, try to consume a prefix.</li>\n",
    "    <li>If we consumed a prefix, go back to the beginning of the loop, so that special-cases always get priority.</li>\n",
    "    <li>If we didn't consume a prefix, try to consume a suffix.</li>\n",
    "    <li>If we can't consume a prefix or suffix, look for \"infixes\" — stuff like hyphens etc.</li>\n",
    "    <li>Once we can't consume any more of the string, handle it as a single token.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(spacy_model.vocab)\n",
    "\n",
    "tokenized_words_train = [tokenizer(sent) for sent in train_text]\n",
    "tokenized_words_test = [tokenizer(sent) for sent in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tokenized_words_spacy_train', tokenized_words_train)\n",
    "np.save('tokenized_words_spacy_test', tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = np.load('tokenized_words_spacy_train.npy')\n",
    "tokenized_words_test = np.load('tokenized_words_spacy_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "tokenized_words_train = [[word for word in sent if word.is_alpha] for sent in tokenized_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ASCII characters\n",
    "tokenized_words_train_flat = [item for sublist in tokenized_words_train for item in sublist]\n",
    "\n",
    "cleaned_data = [re.sub(r'[^\\x00-\\x7f]', r'', word.text) for word in tokenized_words_train_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low-frequency words\n",
    "freq_words = Counter(cleaned_data)\n",
    "\n",
    "cleaned_data = { key : value for key, value in freq_words.items() if value > 10 }\n",
    "\n",
    "filtered_data = []\n",
    "temp_array = []\n",
    "\n",
    "for sent in tokenized_words_train:\n",
    "    for word in sent:\n",
    "        if word.text in cleaned_data.keys():\n",
    "            temp_array.append(word)\n",
    "    filtered_data.append(temp_array)\n",
    "    temp_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Caution!</b> Removing the stop words might not be the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "# filtered_data_no_stopwords = [[word for word in sent if word.is_stop == False] for sent in filtered_data]\n",
    "\n",
    "# filtered_data = filtered_data_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "filtered_data_no_lemma = [[word.lemma_ for word in sent] for sent in filtered_data]\n",
    "\n",
    "filtered_data = filtered_data_no_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "<br>\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "<br>\n",
    "It tries to make words with similar context occupy close spatial positions.\n",
    "<br><br>\n",
    "The Word2Vec model can be obtained using 2 techniques: \n",
    "<ol>\n",
    "    <li>Skip Gram</li>\n",
    "    <li>Common Bag Of Words (CBOW)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_wiki = KeyedVectors.load_word2vec_format('../data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[embed_wiki[word] for word in sent if word in embed_wiki.vocab] for sent in filtered_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of the vectors\n",
    "X_avg = []\n",
    "\n",
    "for vector in X:\n",
    "    if len(vector) >= 1:\n",
    "        X_avg.append(np.mean(vector, axis=0))\n",
    "    else:\n",
    "        X_avg.append(np.zeros(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_avg = np.array(X_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross valudation\n",
    "LR = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    LR, \n",
    "    X_avg, \n",
    "    train_labels, \n",
    "    cv = 5, \n",
    "    scoring = 'f1_macro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6377160304145425"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = np.sum(scores) / len(scores)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_text[:100]\n",
    "\n",
    "tokenized_text = [spacy_model(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tagging.png\" alt=\"tagging\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech tagging\n",
    "tagged_text = [{word : word.tag_ for word in sent} for sent in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads.\n",
    "<br>\n",
    "<img src=\"images/parsing.png\" alt=\"parsing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get noun chunks\n",
    "\n",
    "sample_sent = tokenized_text[1]\n",
    "\n",
    "text = []\n",
    "root = []\n",
    "root_dep = []\n",
    "root_head = []\n",
    "\n",
    "for chunk in sample_sent.noun_chunks:\n",
    "    text.append(chunk.text)\n",
    "    root.append(chunk.root.text)\n",
    "    root_dep.append(chunk.root.dep_)\n",
    "    root_head.append(chunk.root.head.text)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "        'TEXT': text, \n",
    "        'ROOT.TEXT': root, \n",
    "        'ROOT.DEP': root_dep, \n",
    "        'ROOT.HEAD.TEXT': root_head\n",
    "    })\n",
    "\n",
    "df = df[['TEXT', 'ROOT.TEXT', 'ROOT.DEP', 'ROOT.HEAD.TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_sent)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Text</b>: The original noun chunk text.\n",
    "<br>\n",
    "<b>Root text</b>: The original text of the word connecting the noun chunk to the rest of the parse.\n",
    "<br>\n",
    "<b>Root dep</b>: Dependency relation connecting the root to its head.\n",
    "<br>\n",
    "<b>Root head text</b>: The text of the root token's head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.\n",
    "<br>\n",
    "In the image below, we can see the entity types that spaCy supports\n",
    "\n",
    "<img src=\"images/entity_types.png\" alt=\"entity_types\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "sample_sent = 'Google was founded in 1998 in California'\n",
    "doc = spacy_model(sample_sent)\n",
    "\n",
    "text = []\n",
    "start = []\n",
    "end = []\n",
    "label = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    text.append(ent.text)\n",
    "    start.append(ent.start_char)\n",
    "    end.append(ent.end_char)\n",
    "    label.append(ent.label_)\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    'TEXT': text, \n",
    "    'START': start, \n",
    "    'END': end, \n",
    "    'LABEL': label\n",
    "})\n",
    "\n",
    "df = df[['TEXT', 'START', 'END', 'LABEL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'dog cat chair'\n",
    "doc = spacy_model(text)\n",
    "\n",
    "token1 = doc[0]\n",
    "token2 = doc[1]\n",
    "token3 = doc[2]\n",
    "\n",
    "\n",
    "print('Similarity between {0} and {1} is: {2}'.format(token1, token2, token1.similarity(token2)))\n",
    "print('Similarity between {0} and {1} is: {2}'.format(token1, token3, token1.similarity(token3)))\n",
    "print('Similarity between {0} and {1} is: {2}'.format(token1, token1, token1.similarity(token1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Spacy model\n",
    "spacy_model = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_text[:100]\n",
    "\n",
    "tokenized_text = [spacy_model(word) for word in text]\n",
    "\n",
    "text = []\n",
    "has_vector = []\n",
    "vector_norm = []\n",
    "\n",
    "tokenized_text\n",
    "\n",
    "for sent in tokenized_text:\n",
    "    for word in sent:\n",
    "        text.append(word.text)\n",
    "        has_vector.append(word.has_vector)\n",
    "        vector_norm.append(word.vector_norm)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame({\n",
    "        'TEXT': text, \n",
    "        'HAS VECTOR': has_vector, \n",
    "        'VECTOR NORM': vector_norm, \n",
    "    })\n",
    "\n",
    "df = df[['TEXT', 'HAS VECTOR', 'VECTOR NORM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_word_spacy(word, etypes=set([u'MONEY', u'DATE',\n",
    "                                           u'TIME',\n",
    "                                           u'CARDINAL',\n",
    "                                           u'PERCENT'])):\n",
    "    \"\"\"\n",
    "    Returns the word or its representation (unicode)\n",
    "    Args:\n",
    "        word (spacy word): word from spacy\n",
    "        etypes (set): set of words for different entities\n",
    "    Returns:\n",
    "        spacy token represented in different format\n",
    "    \"\"\"\n",
    "    \n",
    "    if word.like_url:\n",
    "        return 'url'\n",
    "    elif word.ent_type_ in etypes:\n",
    "        return word.ent_type_.lower()\n",
    "    elif u'%' in word.lemma_:\n",
    "        return u'percent'\n",
    "    elif word.like_num:\n",
    "        return u'cardinal'\n",
    "    elif (\"dd\" and \":\") in word.shape_:\n",
    "        return u'time'\n",
    "    elif \"$\" in word.text:\n",
    "        return re.sub('\\d','d', word.text)\n",
    "    # checks if the string is date \n",
    "    elif ((\"d/d/d\" or \"d-d-d\" or \"d.d.d\") in re.sub('\\d+', 'd', word.text)):\n",
    "        return u'date'\n",
    "    else:\n",
    "        return (word.lemma_).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "temp = []\n",
    "\n",
    "for sent in tokenized_text:\n",
    "    for word in sent:\n",
    "        temp.append(represent_word_spacy(word))\n",
    "    cleaned_data.append(temp)\n",
    "    temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
